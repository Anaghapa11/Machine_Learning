{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8688f12-c0c6-4d42-8394-ce1f1bde7155",
   "metadata": {},
   "source": [
    " # Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b70b937-e209-4f42-9ae0-effae9434593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "896d849d-a330-4d56-8c6a-8f5486a07963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "missing_values = X.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n",
    "\n",
    "#The Breast Cancer dataset from sklearn does not contain any missing values.\n",
    "#However, it is good practice to check for them before applying any ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cccfc8-a343-4077-b57d-2e00a925b367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Missing Values Handling:\n",
    "We checked for missing values to ensure the integrity of the dataset.\n",
    "As there are no missing values, no imputation was needed. If missing values were present, strategies like mean/median imputation would be used.\n",
    "\n",
    "#Feature Scaling:\n",
    "We applied StandardScaler to scale the features to have a mean of 0 and standard deviation of 1.\n",
    "Scaling is important for algorithms like Logistic Regression, SVM, KNN, and Gradient Descent-based models since they are sensitive to the magnitude of input features.\n",
    "It ensures faster convergence and better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44214103-45f0-4739-9238-ad929cecafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the features\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252de4c1-07f6-4904-bc49-3664bb850a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdafc0b3-38f1-42d3-ba59-b2399b5a04a0",
   "metadata": {},
   "source": [
    "# Classification Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb26dcb1-225c-468b-8e9a-66bcf851e12a",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression is a linear model used for binary classification. It models the probability that a given input belongs to a certain class using the sigmoid function.\n",
    "\n",
    "#Why Suitable:\n",
    "Works well for linearly separable classes.\n",
    "Fast and interpretable for binary classification problems like this one (malignant vs. benign)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1436089b-e17f-42b8-b3d4-d2bc1ffe2614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fcc495-6465-40ad-938d-69f31d15066d",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier\n",
    "\n",
    "A decision tree splits the data based on feature values to create branches that lead to decision outcomes (class labels). It learns rules from data by recursively splitting it.\n",
    "\n",
    "#Why Suitable:\n",
    "Captures non-linear relationships.\n",
    "Easy to visualize and interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a8006e7-6ec0-42d7-b2b6-6d11520e8541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a220161-c591-4414-a6e2-3c3b45db29eb",
   "metadata": {},
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "An ensemble method that builds multiple decision trees and combines their outputs (via majority voting) to improve accuracy and reduce overfitting.\n",
    "\n",
    "#Why Suitable:\n",
    "Handles high-dimensional data well.\n",
    "More robust than a single decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3ccef3a-2a9e-49b3-8fc9-043eb00d094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea64417-5832-4c84-93ba-89c4cea7b107",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)\n",
    "\n",
    "SVM finds the hyperplane that best separates the classes with the maximum margin. It can use different kernel functions to model complex decision boundaries.\n",
    "\n",
    "#Why Suitable:\n",
    "Effective in high-dimensional spaces.\n",
    "Performs well on binary classification problems with clear margins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed8aac8d-0842-42e3-abf5-40c5d966bac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89688b6f-603e-4b69-8c14-552901a659b6",
   "metadata": {},
   "source": [
    "#  k-Nearest Neighbors (k-NN)\n",
    "k-NN is a lazy learner that assigns class labels based on the majority vote of the k nearest training examples in the feature space.\n",
    "\n",
    "#Why Suitable:\n",
    "Simple and effective for smaller datasets.\n",
    "No explicit training, good for baseline comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dee149e-b916-4258-8c58-ca101836ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-NN Accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "print(\"k-NN Accuracy:\", accuracy_score(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe48466-fc8f-4ac1-859d-52dc31c4fc6a",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d090faa3-47de-4981-9d1f-a0bf6be5b293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:       0.9736842105263158\n",
      "Decision Tree Accuracy:             0.9473684210526315\n",
      "Random Forest Accuracy:             0.9649122807017544\n",
      "Support Vector Machine Accuracy:    0.9736842105263158\n",
      "k-Nearest Neighbors Accuracy:       0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# Accuracy scores for all models\n",
    "print(\"Logistic Regression Accuracy:      \", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Decision Tree Accuracy:            \", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Random Forest Accuracy:            \", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Support Vector Machine Accuracy:   \", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"k-Nearest Neighbors Accuracy:      \", accuracy_score(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce69d2f-7bc4-412b-897a-de33877dd568",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best Performing Algorithm\n",
    "Support Vector Machine (SVM) showed the highest accuracy (97.37% in this run).\n",
    "It is especially effective on small- to medium-sized datasets with clear margins between classes, which applies to the Breast Cancer dataset.\n",
    "\n",
    "#Worst Performing Algorithm\n",
    "Decision Tree Classifier had the lowest accuracy (~93%).\n",
    "This could be due to overfitting, as decision trees tend to memorize the training data, especially without pruning.\n",
    "\n",
    "#Conclusion\n",
    "SVM and Random Forest are the top performers, providing high accuracy and robustness.\n",
    "Logistic Regression also performs very well, confirming the linear separability of the dataset.\n",
    "Decision Tree is the weakest performer here, possibly due to overfitting or high variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
